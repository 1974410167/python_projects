#urllib

包含模块
     -urllib.request:打开和读取urls
     -urllib.error:包含urllib.request产生的常见错误，使用try捕捉
     -urllib.parse:包含解析url的方法
     -urllib.robotparse:解析robots.txt文件
     -案列v1
网页编码问题解决
     -chardet 可以自动检测页面的编码格式，但是，可能有误
     -需要安装，conda install chardet
     -案例v2
uplopen的返回对象
    -geturl:返回对象的url
    -info:请求反馈对象的meta信息
    -getcode:返回的http code
    -案例v3
request.date的使用
    -访问网络的两种方法
        -get:
            -利用参数给服务器传递信息
            -参数为dict,然后用parse编码
            -案例v4
        -post：
            -一般向服务器传递参数使用
            -post是把信息自动加密处理
            -使用post.意味着http的请求头可能需要更改：
                -Content-Type:application/x-www.form-urlencode
                -Content-Length:数据长度
                -简而言之，一旦更改请求方法，请注意与其他请求头部信息相对应
            -urllib.parse.urlencode可以将字符串自动转换
            -案例v5
            -为了更多的设置请求信息，单纯的他通过urlopen函数已经不太好用
            -需要利用request.Request 类
            -案例v6
-urllib.error
    -URLError产生的原因：
        -没网
        -服务器连接失败
        -找不到指定的服务器
        -是OSError的子类
        -案例v7
    -HTTPError，是URLError的一个子类
        -案例v8
    -两者的区别：
        -HTTPError是对应的HTTP请求的返回码错误，如果返回的错误码是400以上，则引发HTTPError
        -URLError对应的一般是网络出现问题，包括url问题
        -从属关系：OSError-URLError-HTTPError
-UserAgent
    -UserAgent:用户代理，简称UA，属于heads的一部分，服务器通过UA来判断访问者的身份
    -常见的UA值(可以直接百度搜索)，使用的时候可以直接粘贴复制，也可以用浏览器访问的时候抓包
        IPhone
        Mozilla/5.0 (iPhone; U; CPU iPhone OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5

        IPAD
        Mozilla/5.0 (iPad; U; CPU OS 4_2_1 like Mac OS X; zh-cn) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8C148 Safari/6533.18.5
        Mozilla/5.0 (iPad; U; CPU OS 4_3_3 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8J2 Safari/6533.18.5

        Android
        Mozilla/5.0 (Linux; U; Android 2.2.1; zh-cn; HTC_Wildfire_A3333 Build/FRG83D) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1
        Mozilla/5.0 (Linux; U; Android 2.3.7; en-us; Nexus One Build/FRF91) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1

        QQ浏览器 Android版本
        MQQBrowser/26 Mozilla/5.0 (Linux; U; Android 2.3.7; zh-cn; MB200 Build/GRJ22; CyanogenMod-7) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1

        Android Opera Mobile
        Opera/9.80 (Android 2.3.4; Linux; Opera Mobi/build-1107180945; U; en-GB) Presto/2.8.149 Version/11.10
    -设置UA可以通过两种方式
        -heads
        -add_header
        -案例V9
-ProxyHandler处理(代理服务器)
    -使用代理IP，是爬虫的常用手段
    -获取代理服务器的地址：
        -www.xicidaili.com
        -www.goubanjia.com
    -代理用来隐藏真实访问，代理也不允许频繁的访问某一个固定网站，所以，要用很多代理
    -基本使用步骤:
        1.设置代理地址
        2.创建ProxyHandler
        3.创建Opener
        4.安装Opener
    -案例v10
-cookie & session
    -由于http协议的无记忆性，人为的弥补这一缺陷，所采用的一个补充协议
    -cookie是发放给用户(即http浏览器的)的一段信息，seeeion是保存在服务器上的对应的另一半信息，用来记录用户信息
    -cookie和session的区别
        -存放位置不同
        -cookie不安全
        -session会保存在服务器上一段时间，会过期
        -单个cookie保存的数据不超过4K，很多浏览器限制一个站点最多保存20个
    -session的存放位置
        -存在服务器
        -一般情况，session是放在内存中或者数据库中
-没有cookie登陆
    -案例V11，直接登陆QQ空间，可以看到，没使用cookie则反馈网页为未登录状态
-使用cookie登陆
    -直接把cookie复制下来，然后手动放入代码案例v12
    -http模块包含一些关于cookie的模块，通过他们我们可以自动使用cookie
        -CookieJar
            -管理存储cookie,向传出的http请求添加cookie
            -cookie存储在内存中，CookieJar实例回收后cookie消失
        -FileCookieJar(filename,delayload=None,policy=None):
            -使用文件管理cookie
            -filename是保存cookie的文件
        -MozillaCookieJar(filename,delayload=None,policy=None):
            -创建与mocilla浏览器cookie.txt兼容的FileCookieJar实例
        -LwpCookieJar
            -创建与libwww-perl标准兼容的Set-Cookie3格式的FileCookieJar实例
        -他们之间的关系-CookieJar-->FileCookieJar-->MozillaCookieJar & LwpCookieJar
    -利用cookiejar访问人人网
        -自动使用cookie登陆，大致流程
            -打开登陆页面后自动通过用户名密码登陆
            -自动提取反馈回来的cookie
            -利用提取的cookie登陆隐私页面
            -v13
    -handler是Handler的实例，常用参看案例代码
        -用来处理复杂的请求
            #生成cookie的管理器
            cookie_handler=request.HTTPCookieProcessor(cookie)
            #创建http请求管理器
            http_handler=request.HTTPHandler()
            #创建https管理器
            https_handler=request.HTTPSHandler()
        -创立handler后，使用opener打开，打开后由相应的handler处理
        -cookie作为变量，打印出来，案例v14
            -cookie的属性
                -name:名称
                -value:值
                -domain:可以访问此cookie的域名
                -path:可以访问此cookie的页面路径
                -expires:过期时间
                -size:大小
                -Http字段
        -cookie的保存-FileCookieJar,案例v15
        -cookie的读取，案例v16
-SSL
    -SSL证书是指遵守SSL安全套阶层协议的服务器数字证书(SercureSocketLayer)
    -美国网景公司开发
    -CA(CertifacateAuthority)是数字证书认证中心，是发放，管理，废除数字证书的收信人的第三方机构
    -遇到不信任的SSL证书，需要单独处理，案例v17
-js加密
    -有的反爬虫策略采用js对需要传输的数据进行加密处理(通常取md5值)
    -经过加密，传输的就是密文
    -加密函数或者过程一定是在浏览器完成，也就是一定会把代码(js代码)暴露给使用者
    -通过阅读加密算法，就可以模拟出加密过程，从而达到破解
-ajax
    -异步请求
    -一定会用url,请求方法，可能有数据
    -一般使用json格式
    -案例，爬取豆瓣电影，案例v20
-requests-献给人类的
    -HTTP for Humans,更加简洁更友好
    -继承了urllib的所有特征
    -底层使用的是urllib3
    -开源地址：http://github.com/requests/requests
    -中文文档：http://docs.python-requests.org/zh_CN/latest/index.html
    -get请求
        -requests.get(url)
        -requests.request("get",url)
        -可以带有hearders参数和parmas参数
        -get返回内容
        -v22
    -post
        -rsp=requests.post(url,data=data)
        -data,headers要求dict类型
        -案例v23
    -proxy代理
        -
            proxies={
                "http":"address of proxy",
                "https":"address of proxy"
                }
                rep=requests.request("get","http:xxxxxxx",proxies=proxies)
        -代理有可能报错，如果使用人数多，考虑安全问题，可能会被强行关闭
    -用户验证
        -代理验证
            #可能需要使用HTTP basic Auth,可以这样
            #格式为 用户名：密码@代理地址：端口地址
            proxy={"http":"用户名:密码@192.168.1.123：4444"}
            rsp=requests.get("http://baidu.com",proxies=proxy)
    -Web客户端验证
        -如果遇到web客户端验证，需要添加auth=(用户名，密码)
            auth=("用户名","密码")#授权信息
            rsp=requests.get("http://www.baidu.com",auth=auth)
    -cookie
        -requests可以自动处理cookie的信息
            rep=requests.get("http://xxxxxxxxxxxxxx")
            #如果对方服务器传送过来cookie信息，则可以通过反馈的cookie属性得到
            #返回cookiejar实例
            cookiejar=rep.cookies

            #可以将cookeijar转换为字典
            cookiedict=requests.utils.dict_from_cookiejar(cookiejar)
    -session
        -跟服务器session不是一个东西
        -模拟一次会话，从客户端浏览器链接服务器开始，到客户端浏览器断开
        -能让我们跨请求时保持某些参数，比如再同一个session实例发出 所有请求之间保持cookie

            #创建session对象，可以保持cookie值
            ss=requests.session()

            headers={"User-Agent":"xxxxxxxxxxxxxx"}

            data={"name":"xxxxxxxxxxxxxx"}

            #此时，由创建的session管理请求，负责发出请求
            ss.post("http://www.baidu.com",data=data,headers=headers)

            rep=ss.get("xxxxxxxxxxx")
    -https请求验证ssl证书
        -参数verify负责表示是否需要验证ss证书，默认是True即需要
        -如果不需要验证ssl证书，则设置成False表示关闭

            rep=requests.get("https://www.baidu.com",verify=False)
            #如果用verify=True访问12306，会报错，因为他证书有问题


            


